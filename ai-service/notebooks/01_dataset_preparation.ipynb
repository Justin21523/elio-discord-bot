{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865cf54c",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Finetuning,\n",
    "This notebook handles:,\n",
    "- Loading raw data\\,\n",
    "- Formatting for different tasks (SFT, DPO, Persona, Story)\\,\n",
    "- Train/validation splitting,\n",
    "- Tokenization and saving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4478b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\\n\",\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"  # or Qwen, Llama, etc.\n",
    "DATASET_PATH = \"/mnt/c/ai_warehouse/datasets/OpenAssistant___oasst2\"\n",
    "OUTPUT_DIR = \"./models/dialogue_sft\"\n",
    "MAX_LENGTH = 2048\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72120f84",
   "metadata": {},
   "source": [
    "## Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cfba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dbe9b6f4f145109651043668a15c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\b0979\\.cache\\huggingface\\hub\\datasets--OpenAssistant--oasst2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0df02ae2244da18ba758f61e40d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-88ba0162028a73(…):   0%|          | 0.00/63.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3885263fdab247f6ac3594137bc2443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-1deeef95c(…):   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42001c2c238c4f66943734fda3638428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/128575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d72abab25f841fe9b3d214cd9f5ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/6599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 載入 OASST2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAssistant/oasst2\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/c/ai_warehouse/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\b0979\\miniconda3\\envs\\env-ai\\lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 載入 OASST2\n",
    "dataset = load_dataset(\"OpenAssistant/oasst2\", cache_dir=\"/mnt/c/ai_warehouse/datasets\")\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd54374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\\n\",\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Average prompt length: {df['prompt'].str.len().mean():.0f} chars\")\n",
    "print(f\"Average response length: {df['response'].str.len().mean():.0f} chars\")\n",
    "\n",
    "# Distribution of lengths\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['prompt'].str.len(), bins=50)\n",
    "plt.xlabel('Prompt Length (chars)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prompt Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['response'].str.len(), bins=50)\n",
    "plt.xlabel('Response Length (chars)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Response Length Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd31ef3",
   "metadata": {},
   "source": [
    "## Format Data for SFT (Supervised Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_sft(examples):\n",
    "    \"\"\"\n",
    "    Format data for supervised fine-tuning\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        # Chat template format\\n\",\n",
    "        text = f\"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant for the Elio/Pixar community.<|im_end|>\n",
    "<|im_start|>user\n",
    "{examples['prompt'][i]}<|im_end|>\n",
    "<|im_start|>assistant\\n\",\n",
    "{examples['response'][i]}<|im_end|>\"\"\"\n",
    "        formatted.append(text)\n",
    "\n",
    "    return {'text': formatted}\n",
    "\n",
    "# Apply formatting\n",
    "df_sft = df.copy()\n",
    "formatted_data = format_for_sft({\n",
    "    'prompt': df_sft['prompt'].tolist(),\n",
    "    'response': df_sft['response'].tolist()\n",
    "})\n",
    "\n",
    "df_sft['text'] = formatted_data['text']\n",
    "print(f\"\\nFormatted {len(df_sft)} samples for SFT\")\n",
    "print(\"\\nExample:\")\n",
    "print(df_sft['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee249d",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\\n\",\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "# Create HuggingFace Dataset\\n\",\n",
    "dataset = Dataset.from_pandas(df_sft[['text']])\n",
    "\n",
    "# Tokenize\\n\",\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06177fc",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\\n\",\n",
    "split_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e049a8",
   "metadata": {},
   "source": [
    "## Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50076c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatasetDict\\n\",\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Save to disk\\n\",\n",
    "output_path = os.path.join(OUTPUT_DIR, 'sft_dataset')\n",
    "dataset_dict.save_to_disk(output_path)\n",
    "print(f\"\\nDataset saved to: {output_path}\")\n",
    "\n",
    "# Also save metadata\\n\",\n",
    "metadata = {\n",
    "    'base_model': BASE_MODEL,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'train_samples': len(train_dataset),\n",
    "    'val_samples': len(val_dataset),\n",
    "    'validation_split': VALIDATION_SPLIT,\n",
    "    'format': 'sft',\n",
    "    'created_at': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'dataset_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba39bfa",
   "metadata": {},
   "source": [
    "## Token Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af538380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token lengths\\n\",\n",
    "token_lengths = [len(x['input_ids']) for x in train_dataset]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.axvline(MAX_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_LENGTH})')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Token Lengths in Training Set')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nToken Length Statistics:\")\n",
    "print(f\"Mean: {pd.Series(token_lengths).mean():.0f}\")\n",
    "print(f\"Median: {pd.Series(token_lengths).median():.0f}\")\n",
    "print(f\"95th percentile: {pd.Series(token_lengths).quantile(0.95):.0f}\")\n",
    "print(f\"Max: {pd.Series(token_lengths).max():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
