{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with LoRA/QLoRA - FIXED VERSION\n",
    "\n",
    "This notebook covers:\n",
    "- Loading base model and dataset from your datasets folder\n",
    "- Configuring LoRA/QLoRA for efficient training\n",
    "- Training with Trainer API\n",
    "- Evaluation and metrics\n",
    "- Saving and merging adapters\n",
    "\n",
    "**Updated to work with your environment and datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PATHS ==========\n",
    "# Shared dataset location\n",
    "DATASETS_DIR = \"/mnt/c/AI_LLM_projects/ai_warehouse/datasets\"\n",
    "\n",
    "# Bot project paths (for Communiverse data)\n",
    "BOT_DATA_DIR = \"/mnt/c/web-projects/elioverse-bot/data\"\n",
    "TRAINING_DATA_DIR = os.path.join(BOT_DATA_DIR, \"training-datasets\")\n",
    "\n",
    "# Model paths\n",
    "BASE_MODEL = \"deepseek-ai/deepseek-llm-7b-base\"  # Adjust based on what you downloaded\n",
    "OUTPUT_DIR = \"../models/sft_lora_communiverse\"\n",
    "\n",
    "# ========== DATASET SELECTION ==========\n",
    "# Choose which datasets to use for training\n",
    "USE_DATASETS = {\n",
    "    'oasst2': True,          # OpenAssistant high-quality dialogues\n",
    "    'ultrachat': False,      # Large scale conversations (set False to save time)\n",
    "    'alpaca': True,          # Instruction following\n",
    "    'firefly': True,         # Chinese + English mix\n",
    "    'communiverse': True,    # Your custom Communiverse data\n",
    "}\n",
    "\n",
    "# ========== LORA CONFIGURATION ==========\n",
    "LORA_R = 16  # Rank (8, 16, 32 are common)\n",
    "LORA_ALPHA = 32  # Scaling factor (typically 2x rank)\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # Attention\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"       # FFN\n",
    "]\n",
    "\n",
    "# ========== TRAINING HYPERPARAMETERS ==========\n",
    "BATCH_SIZE = 2                    # Per device batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 8   # Effective batch = 2 * 8 = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# ========== OPTIMIZATION SETTINGS ==========\n",
    "USE_4BIT = True          # QLoRA (recommended for 24GB VRAM)\n",
    "USE_8BIT = False         # Alternative for larger VRAM\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "USE_FP16 = not USE_4BIT  # Auto-disable if using 4-bit\n",
    "USE_BF16 = False         # Use if you have A100/H100\n",
    "\n",
    "# ========== DATA LIMITS (for testing) ==========\n",
    "MAX_TRAIN_SAMPLES = None  # Set to small number (e.g., 1000) for testing\n",
    "MAX_VAL_SAMPLES = None    # Set to small number (e.g., 100) for testing\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oasst2_dataset():\n",
    "    \"\"\"Load OpenAssistant OASST2 dataset\"\"\"\n",
    "    print(\"Loading OASST2...\")\n",
    "    dataset_path = os.path.join(DATASETS_DIR, \"OpenAssistant___oasst2\")\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        dataset = load_from_disk(dataset_path)\n",
    "        # OASST2 has train/validation splits\n",
    "        return dataset\n",
    "    else:\n",
    "        print(f\"  Downloading from HuggingFace...\")\n",
    "        dataset = load_dataset(\"OpenAssistant/oasst2\")\n",
    "        return dataset\n",
    "\n",
    "def load_alpaca_dataset():\n",
    "    \"\"\"Load Alpaca dataset\"\"\"\n",
    "    print(\"Loading Alpaca...\")\n",
    "    dataset_path = os.path.join(DATASETS_DIR, \"tatsu-lab___alpaca\")\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        dataset = load_from_disk(dataset_path)\n",
    "        return dataset\n",
    "    else:\n",
    "        print(f\"  Downloading from HuggingFace...\")\n",
    "        dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "        return dataset\n",
    "\n",
    "def load_firefly_dataset():\n",
    "    \"\"\"Load Firefly Chinese dataset\"\"\"\n",
    "    print(\"Loading Firefly...\")\n",
    "    dataset_path = os.path.join(DATASETS_DIR, \"YeungNLP___firefly-train-1.1_m\")\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        dataset = load_from_disk(dataset_path)\n",
    "        return dataset\n",
    "    else:\n",
    "        print(f\"  Downloading from HuggingFace...\")\n",
    "        dataset = load_dataset(\"YeungNLP/firefly-train-1.1_m\")\n",
    "        return dataset\n",
    "\n",
    "def load_communiverse_dataset():\n",
    "    \"\"\"Load your custom Communiverse training data\"\"\"\n",
    "    print(\"Loading Communiverse custom data...\")\n",
    "    \n",
    "    # Try to load from training-datasets folder\n",
    "    jsonl_path = os.path.join(TRAINING_DATA_DIR, \"communiverse_training.jsonl\")\n",
    "    json_path = os.path.join(TRAINING_DATA_DIR, \"communiverse_training.json\")\n",
    "    \n",
    "    if os.path.exists(jsonl_path):\n",
    "        dataset = load_dataset('json', data_files=jsonl_path)\n",
    "        return dataset['train']  # JSONL format returns 'train' split\n",
    "    elif os.path.exists(json_path):\n",
    "        dataset = load_dataset('json', data_files=json_path)\n",
    "        return dataset['train']\n",
    "    else:\n",
    "        print(f\"  WARNING: Communiverse data not found!\")\n",
    "        print(f\"  Expected at: {jsonl_path}\")\n",
    "        print(f\"  Run: node scripts/generate-training-data.js first!\")\n",
    "        return None\n",
    "\n",
    "def format_instruction_dataset(example, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Convert different dataset formats to a unified format.\n",
    "    Returns: {\"text\": \"<formatted instruction-response pair>\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Alpaca format\n",
    "    if 'instruction' in example and 'output' in example:\n",
    "        instruction = example['instruction']\n",
    "        input_text = example.get('input', '')\n",
    "        output = example['output']\n",
    "        \n",
    "        if input_text:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    \n",
    "    # OASST2 format (conversation trees)\n",
    "    elif 'text' in example and dataset_name == 'oasst2':\n",
    "        # OASST2 already has formatted conversations\n",
    "        text = example['text']\n",
    "    \n",
    "    # Firefly format\n",
    "    elif 'input' in example and 'target' in example:\n",
    "        text = f\"### Instruction:\\n{example['input']}\\n\\n### Response:\\n{example['target']}\"\n",
    "    \n",
    "    # Communiverse format (from generate-training-data.js)\n",
    "    elif 'persona' in example and 'dialogue' in example:\n",
    "        persona = example['persona']\n",
    "        dialogue = example['dialogue']\n",
    "        text = f\"### Character: {persona}\\n\\n{dialogue}\"\n",
    "    \n",
    "    # Already formatted\n",
    "    elif 'text' in example:\n",
    "        text = example['text']\n",
    "    \n",
    "    else:\n",
    "        # Fallback: combine all fields\n",
    "        text = str(example)\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected datasets\n",
    "all_datasets = []\n",
    "dataset_stats = {}\n",
    "\n",
    "if USE_DATASETS['oasst2']:\n",
    "    try:\n",
    "        ds = load_oasst2_dataset()\n",
    "        # Take train split\n",
    "        train_ds = ds['train'].map(lambda x: format_instruction_dataset(x, 'oasst2'))\n",
    "        all_datasets.append(train_ds)\n",
    "        dataset_stats['oasst2'] = len(train_ds)\n",
    "        print(f\"  âœ“ OASST2: {len(train_ds):,} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed to load OASST2: {e}\")\n",
    "\n",
    "if USE_DATASETS['alpaca']:\n",
    "    try:\n",
    "        ds = load_alpaca_dataset()\n",
    "        train_ds = ds['train'].map(lambda x: format_instruction_dataset(x, 'alpaca'))\n",
    "        all_datasets.append(train_ds)\n",
    "        dataset_stats['alpaca'] = len(train_ds)\n",
    "        print(f\"  âœ“ Alpaca: {len(train_ds):,} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed to load Alpaca: {e}\")\n",
    "\n",
    "if USE_DATASETS['firefly']:\n",
    "    try:\n",
    "        ds = load_firefly_dataset()\n",
    "        train_ds = ds['train'].map(lambda x: format_instruction_dataset(x, 'firefly'))\n",
    "        # Firefly is large, take subset\n",
    "        if len(train_ds) > 50000:\n",
    "            train_ds = train_ds.select(range(50000))\n",
    "        all_datasets.append(train_ds)\n",
    "        dataset_stats['firefly'] = len(train_ds)\n",
    "        print(f\"  âœ“ Firefly: {len(train_ds):,} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed to load Firefly: {e}\")\n",
    "\n",
    "if USE_DATASETS['communiverse']:\n",
    "    try:\n",
    "        ds = load_communiverse_dataset()\n",
    "        if ds is not None:\n",
    "            train_ds = ds.map(lambda x: format_instruction_dataset(x, 'communiverse'))\n",
    "            all_datasets.append(train_ds)\n",
    "            dataset_stats['communiverse'] = len(train_ds)\n",
    "            print(f\"  âœ“ Communiverse: {len(train_ds):,} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed to load Communiverse: {e}\")\n",
    "\n",
    "# Combine all datasets\n",
    "if len(all_datasets) == 0:\n",
    "    raise ValueError(\"No datasets loaded! Please check your configuration.\")\n",
    "\n",
    "print(f\"\\nCombining {len(all_datasets)} datasets...\")\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "\n",
    "# Shuffle\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "# Apply limits if testing\n",
    "if MAX_TRAIN_SAMPLES:\n",
    "    train_dataset = train_dataset.select(range(min(MAX_TRAIN_SAMPLES, len(train_dataset))))\n",
    "if MAX_VAL_SAMPLES:\n",
    "    val_dataset = val_dataset.select(range(min(MAX_VAL_SAMPLES, len(val_dataset))))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, count in dataset_stats.items():\n",
    "    print(f\"{name:20s}: {count:>10,} samples\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Total':20s}: {len(combined_dataset):>10,} samples\")\n",
    "print(f\"{'Training':20s}: {len(train_dataset):>10,} samples (95%)\")\n",
    "print(f\"{'Validation':20s}: {len(val_dataset):>10,} samples (5%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training sample:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\" if len(train_dataset[0]['text']) > 500 else train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text and prepare for causal language modeling\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,  # Will be done by data collator\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"\\nTokenizing datasets... (this may take a while)\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training set\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization complete!\")\n",
    "print(f\"Training samples: {len(tokenized_train):,}\")\n",
    "print(f\"Validation samples: {len(tokenized_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {BASE_MODEL}\")\n",
    "print(f\"Quantization: {'4-bit (QLoRA)' if USE_4BIT else '8-bit' if USE_8BIT else 'None (Full precision)'}\")\n",
    "\n",
    "model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
    "\n",
    "if USE_4BIT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model_kwargs[\"quantization_config\"] = bnb_config\n",
    "\n",
    "elif USE_8BIT:\n",
    "    model_kwargs[\"load_in_8bit\"] = True\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **model_kwargs)\n",
    "\n",
    "# Prepare for k-bit training if quantized\n",
    "if USE_4BIT or USE_8BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"Model prepared for quantized training\")\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Total parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Calculate trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LORA CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Rank (r):           {LORA_R}\")\n",
    "print(f\"Alpha:              {LORA_ALPHA}\")\n",
    "print(f\"Dropout:            {LORA_DROPOUT}\")\n",
    "print(f\"Target modules:     {', '.join(LORA_TARGET_MODULES)}\")\n",
    "print(f\"\\nTrainable params:   {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "print(f\"Total params:       {total_params:,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total steps\n",
    "total_steps = (len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=USE_FP16,\n",
    "    bf16=USE_BF16,\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    optim=\"paged_adamw_8bit\" if USE_4BIT else \"adamw_torch\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    ddp_find_unused_parameters=False if USE_GRADIENT_CHECKPOINTING else None,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Epochs:             {NUM_EPOCHS}\")\n",
    "print(f\"Batch size:         {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Learning rate:      {LEARNING_RATE}\")\n",
    "print(f\"Total steps:        {total_steps:,}\")\n",
    "print(f\"Warmup steps:       {int(total_steps * WARMUP_RATIO):,}\")\n",
    "print(f\"Max seq length:     {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Precision:          {'BF16' if USE_BF16 else 'FP16' if USE_FP16 else 'FP32'}\")\n",
    "print(f\"Gradient checkpoint: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "print(\"Data collator and callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(f\"\\nðŸš€ Starting training...\")\n",
    "print(f\"This will take approximately {total_steps * 2 / 3600:.1f} hours (estimate)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training time:      {train_result.metrics['train_runtime'] / 3600:.2f} hours\")\n",
    "print(f\"Training loss:      {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Samples/second:     {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key:25s}: {value:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"âœ“ LoRA adapters saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"datasets\": dataset_stats,\n",
    "    \"total_samples\": len(train_dataset) + len(val_dataset),\n",
    "    \"training_samples\": len(train_dataset),\n",
    "    \"validation_samples\": len(val_dataset),\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": LORA_TARGET_MODULES\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "        \"quantization\": \"4-bit\" if USE_4BIT else \"8-bit\" if USE_8BIT else \"none\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"train_loss\": float(train_result.metrics['train_loss']),\n",
    "        \"eval_loss\": float(eval_results['eval_loss']),\n",
    "        \"train_runtime_hours\": float(train_result.metrics['train_runtime'] / 3600)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training metadata saved\")\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"Testing fine-tuned model...\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nDescribe Elio's personality.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWhat is the Communiverse?\\n\\n### Response:\\n\",\n",
    "    \"### Character: Glordon\\n\\nHuman: What do you think about potatoes?\\n\\nGlordon:\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PROMPT:\\n{prompt}\")\n",
    "    print(f\"\\nRESPONSE:\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from output\n",
    "    response = response[len(prompt):].strip()\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "print(f\"\\nâœ… Training and testing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
