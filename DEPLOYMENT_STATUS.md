# âœ… Production Status & Deployment Guide

**Deployment time:** 2025-10-22
**Mode:** Multi-task Environment Optimized (default)
**Stack:** Discord Bot + AI Service (LLM / VLM / RAG) + MongoDB + Cron Jobs

---

## ğŸ“ˆ Current System Health

```
âœ“ MongoDB        Healthy (Up ~minutes)
âœ“ AI Service     Healthy (Up ~minutes)
âœ“ Discord Bot    Healthy (Up ~seconds)
âœ“ All services registered and running
```

GPU (RTX 5080, 16 GB):

* Temperature: ~41 Â°C (normal)
* Utilization: ~33% during model load

---

## ğŸ§  Deployment Profiles (pick one)

### A) Multi-Task Optimized (default)

* **Embeddings:** Preloaded (~1.5 GB)
* **LLM:** On-demand (first request loads; frees after idle)
* **VLM:** On-demand
* **GPU idle usage:** ~2.2â€“2.3 GB (~14%)
* **Best for:** Running training jobs alongside the bot

**Pros:** ~12.9 GB free at idle, multiple concurrent tasks
**Cons:** First LLM request per warm-up costs ~4â€“6 s

---

### B) Performance Optimized (optional)

* **Embeddings:** Preloaded
* **LLM:** **Preloaded** (8-bit)
* **VLM:** On-demand
* **GPU idle usage:** ~11.2 GB (~69%)
* **Best for:** Heavy chat traffic, minimal cold-start latency

**Pros:** Subsequent replies ~1â€“2 s
**Cons:** Less headroom for training; switch off when not needed

---

## ğŸ”€ Quick Toggle Between Profiles

**Switch to Performance (preload LLM):**

```yaml
# docker-compose.yml (ai-service environment)
PRELOAD_LLM: "true"
PRELOAD_EMBEDDINGS: "true"
USE_8BIT: "true"
USE_4BIT: "false"
FINETUNED_MODEL_ENABLED: "true"
FINETUNED_BASE_MODEL: "deepseek-ai/deepseek-llm-7b-chat"
FINETUNED_ADAPTER_PATH: "/app/models/sft_lora_balanced"
FINETUNED_USE_FOR_PERSONAS: "true"
```

```bash
docker compose up -d --force-recreate ai-service
# wait ~90s for preload
```

**Switch back to Multi-Task (LLM on-demand):**

```yaml
PRELOAD_LLM: "false"
PRELOAD_EMBEDDINGS: "true"
```

```bash
docker compose up -d --force-recreate ai-service
```

> Keep **DeepSeek 7B Chat + LoRA (8-bit)** as your stable baseline.

---

## ğŸ§© Whatâ€™s Implemented (Production)

### Discord Commands (15â€“16)

`/ai, /rag, /persona, /scenario, /game, /minigame, /profile, /leaderboard, /points, /greet, /story, /finetune, /config-proactive, /schedule, /drop, /admin-data`

### Proactive AI (11 cron jobs)

* Auto scenario (4h), media sweep (6h), daily digest (10:00), channel summary (23:00)
* Content expansion (6h), scenario reveal (every minute)
* Auto meme drop (6h), auto persona chat (2h), auto mini-game (4h), auto story weave (noon), auto world builder (midnight)

### Dynamic Data

* Weekly (Sun 03:00): analyze RAG resources â†’ generate personas, scenarios, greetings

---

## ğŸ—‚ï¸ Database Snapshot

```
Personas:   15
Scenarios:  56
Greetings:  37
Indexes:    Ensured (healthy)
JSON:       Validated
```

---

## âš™ï¸ AI Service Configuration (stable)

**.env / compose (key flags):**

```bash
# Model + adapters (stable)
FINETUNED_MODEL_ENABLED=true
FINETUNED_BASE_MODEL=deepseek-ai/deepseek-llm-7b-chat
FINETUNED_ADAPTER_PATH=/app/models/sft_lora_balanced
FINETUNED_USE_FOR_PERSONAS=true

# Quantization (stable)
USE_8BIT=true
USE_4BIT=false

# Preload knobs
PRELOAD_EMBEDDINGS=true
PRELOAD_LLM=false     # default profile (set true for performance profile)
PRELOAD_VLM=false

# RAG
RAG_TOP_K=10
RAG_MIN_SCORE=0.5
```

**PyTorch CUDA allocator (reduces fragmentation):**

```bash
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256,expandable_segments:true
```

---

## ğŸš€ Performance Characteristics

* **RAG only:** ~<0.5 s (embeddings preloaded)
* **LLM (cold):** 4â€“6 s (on-demand load)
* **LLM (warm):** 1â€“2 s
* **VLM (cold/warm):** 8â€“10 s / 2â€“3 s
* **LLM auto-unload:** after ~10 min idle (default profile)

**Multi-task capacity (default):**

```
Idle: ~2.2 GB used (Embeddings only)
Training: 8â€“10 GB + Bot: 2 GB + buffer 2 GB  â†’ ~14 GB / 16 GB
```

---

## ğŸ› ï¸ Post-Fix Summary (what broke & how we fixed)

**Incident:** Switched to Qwen2.5-7B + 4-bit â†’ CUDA allocator crash, 500s on generation.
**Resolution:**

* Revert to **DeepSeek 7B Chat + LoRA** (8-bit)
* `device_map="auto"` for quantized loads
* Keep **Embeddings preloaded**, LLM on-demand (default), or preloaded (performance mode)

Sanity check:

```bash
curl -X POST http://localhost:8000/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Hi","max_tokens":50}'
# => ok: true, model: deepseek
```

---

## ğŸ§ª Production Verification

### Health & Status

```bash
docker compose ps
curl http://localhost:8000/health
docker compose logs -f bot | grep -E "ROUTER|RAG|DM-GAME|JOB|ERR"
```

### Discord Sanity

* Mention bot â†’ reply in ~10s
* Keyword (â€œpotatoâ€) â†’ **Glordon** replies
* Random (2% chance) â†’ occasional reply
* DM: `!persona Elio`, `!game trivia`, `!story space`

### Mini-Game

```
/minigame start type:trivia rounds:3
/minigame stats
/minigame stop
```

### RAG Grounding

* Ask lore question without persona name â†’ reply if similarity â‰¥ threshold
* Adjust sensitivity: `RAG_MIN_SCORE` (0.5 default), router relevance threshold (see below)

---

## ğŸ§  Router & Memory (key behavior)

* Triggers: **mentions** (always), **keywords**, **random 2%**, **images (VLM)**
* Cooldowns: mention (~10 s), keywords (~60 s), random (~120 s)
* Relevance: `RELEVANCE_THRESHOLD=0.3` (env). After first reply, conversation phase threshold ~0.15 to keep thread going.
* Memory: per-channel, per-persona; last ~10 messages; TTL ~30 min; auto-prune

---

## ğŸ“Š Architecture Overview (production view)

```
Discord Gateway
  â”œâ”€ messageCreate (guild)  â†’ src/events/messageCreate.js
  â”‚    â””â”€ messageRouter     â†’ src/services/messageRouter.js
  â”‚         â”œâ”€ triggers + cooldowns
  â”‚         â”œâ”€ conversationHistory (TTL, per persona/channel)
  â”‚         â”œâ”€ RAG (topK=10, minScore=0.5)
  â”‚         â”œâ”€ VLM (if image)
  â”‚         â””â”€ persona compose (finetuned LLM) â†’ ai-service
  â””â”€ dmCreate (DMs)        â†’ src/events/dmCreate.js
       â””â”€ dmHandlers       â†’ src/handlers/dmHandlers.js (chat + games)

Cron scheduler â†’ src/index.js â†’ src/handlers/proactiveHandlers.js â†’ src/jobs/*.js
Webhooks (persona avatars) â†’ src/services/webhooks.js
Persistence: MongoDB (personas, profiles, scenarios, media, dm_sessions, schedules)
Vector store: Atlas Vector Search / FAISS (dev)
```

---

## ğŸ” Monitoring & Ops

**Everyday checks**

```bash
docker compose ps
docker compose logs -f ai-service
docker compose logs -f bot
watch -n 2 'nvidia-smi; echo; docker stats --no-stream'
```

**GPU helper scripts**

```bash
bash scripts/gpu-monitor.sh status   # snapshot
bash scripts/gpu-monitor.sh watch    # live
bash scripts/gpu-monitor.sh cleanup  # free cached
bash scripts/gpu-monitor.sh reset-ai # restart AI service
```

**Service restart**

```bash
docker compose restart [service]
bash scripts/production-deploy.sh  # full redeploy
```

---

## ğŸ§ª Performance & Accuracy Targets

* Avg response: **<3 s** (conversation mode: **<2 s**)
* First-person accuracy (persona): **>75%** (observed ~78.6%)
* Keyword trigger precision: **>90%**
* 24-hour stability: **no crashes, no memory leaks**

Use this table during load tests:

| Scenario    | Message              | Target    |
| ----------- | -------------------- | --------- |
| Cold        | â€œTell me about Elioâ€ | 2â€“3 s     |
| Warm thread | â€œWhat else?â€         | 1â€“2 s     |
| Keyword     | â€œwormhole travelâ€    | 1.5â€“2.5 s |
| RAG-only    | lore question        | <1 s      |

---

## âš ï¸ Do / Donâ€™t (hard rules)

**Do not change (keep stable):**

* `LLM_MODEL` family: DeepSeek 7B Chat (+ LoRA)
* `USE_8BIT=true`, `USE_4BIT=false`
* `FINETUNED_MODEL_ENABLED=true` and adapter path

**Safe to adjust:**

* `PRELOAD_LLM` (toggle profile)
* `RAG_MIN_SCORE`, `RAG_TOP_K`
* `RELEVANCE_THRESHOLD`, cooldowns, max tokens

---

## ğŸ§° Troubleshooting

**AI service â€œnot availableâ€** â†’ Often web-search 429s. Keep `WEB_SEARCH_ENABLED=false` or swap provider.
**Finetuned not loading** â†’ Verify adapter path; logs under `ai-service`.
**No replies** â†’ Check guild `autoReplyChannels`, cooldowns, thresholds.
**OOM / allocator errors** â†’ Reconfirm **8-bit**, disable 4-bit; ensure `device_map="auto"`.

---

## âœ… Production Readiness Checklist

**Health**

* [x] Containers healthy
* [x] Health checks pass
* [x] GPU memory policy set (profile chosen)

**Functionality**

* [x] Commands registered (15â€“16)
* [x] 11 cron jobs active
* [x] RAG + VLM integrated
* [x] Personas seeded (â‰ˆ15)

**Performance**

* [x] Idle GPU <15% (default) or known >60% (perf mode)
* [x] Warm replies â‰ˆ1â€“2 s
* [x] No memory leaks in 24 h soak

---

## ğŸ“ Quick Reference

```bash
# Status & logs
docker compose ps
docker compose logs -f bot
docker compose logs -f ai-service | tail -100
curl http://localhost:8000/health

# GPU
nvidia-smi
bash scripts/gpu-monitor.sh watch

# Discord tests
/minigame start type:trivia rounds:3
/persona ask question:"What is your favorite thing?"
/rag query query:"Tell me about the council"
```

**Docs & files**

* Compose: `docker-compose.yml`
* Scripts: `scripts/`
* AI service config: `ai-service/app/config.py`, `.env`
* Router & memory: `src/services/messageRouter.js`, `src/services/conversationHistory.js`
* Proactive: `src/handlers/proactiveHandlers.js`, `src/jobs/*.js`

---

**Bottom line:**

* Default = **Multi-Task Optimized** (LLM on-demand, embeddings preloaded).
* Flip **one flag** (`PRELOAD_LLM=true`) when you need **Performance Optimized**.
* Stay on **DeepSeek 7B Chat + LoRA (8-bit)** for rock-solid stability.