#!/bin/bash
#
# å®Œæ•´è®­ç»ƒæ•°æ®å‡†å¤‡æµç¨‹
# Complete Training Data Preparation Workflow
#
# æ­¤è„šæœ¬è‡ªåŠ¨åŒ–ä»¥ä¸‹æµç¨‹ï¼š
# 1. ç”Ÿæˆ fandom ç¬¬ä¸€äººç§°æ•°æ®ï¼ˆ1200 samplesï¼‰
# 2. ä»å¤§é‡æ•°æ®é›†ä¸­ç­›é€‰ä¸€èˆ¬å¯¹è¯æ•°æ®ï¼ˆ300 samplesï¼‰
# 3. åˆå¹¶æ•°æ®é›†ï¼ˆ80% fandom + 20% generalï¼‰
# 4. å‡†å¤‡ç”¨äº fine-tuning
#

set -e  # Exit on error

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}============================================${NC}"
echo -e "${BLUE}  å®Œæ•´è®­ç»ƒæ•°æ®å‡†å¤‡æµç¨‹${NC}"
echo -e "${BLUE}============================================${NC}\n"

# æ£€æŸ¥ OpenAI API Key
if [ -z "$OPENAI_API_KEY" ]; then
    echo -e "${YELLOW}âš ï¸  OPENAI_API_KEY not set. Please set it:${NC}"
    echo "export OPENAI_API_KEY='your-key-here'"
    exit 1
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p data/training

# Step 1: ç”Ÿæˆ fandom ç¬¬ä¸€äººç§°æ•°æ®
echo -e "\n${GREEN}[Step 1/3] ç”Ÿæˆ Fandom ç¬¬ä¸€äººç§°è®­ç»ƒæ•°æ®...${NC}"
echo -e "${BLUE}é¢„è®¡æ—¶é—´ï¼š30-45 åˆ†é’Ÿ${NC}\n"

node scripts/generate-fandom-training-data.js

if [ ! -f "data/training/fandom-first-person-training-data.jsonl" ]; then
    echo -e "${YELLOW}âŒ Fandom data generation failed${NC}"
    exit 1
fi

fandom_count=$(wc -l < data/training/fandom-first-person-training-data.jsonl)
echo -e "${GREEN}âœ… Generated ${fandom_count} fandom samples${NC}"

# Step 2: ä»å¤§é‡æ•°æ®é›†ç­›é€‰ä¸€èˆ¬å¯¹è¯æ•°æ®
echo -e "\n${GREEN}[Step 2/3] ç­›é€‰é«˜è´¨é‡ä¸€èˆ¬å¯¹è¯æ•°æ®...${NC}"
echo -e "${BLUE}ä» BelleGroup 0.5M æ•°æ®é›†ä¸­æ™ºèƒ½ç­›é€‰ 300 ä¸ªæ ·æœ¬${NC}\n"

# ä½¿ç”¨ BelleGroup 0.5M ä¸­æ–‡æ•°æ®é›†ï¼ˆè´¨é‡è¾ƒé«˜ï¼‰
BELLE_DATASET="/mnt/c/AI_LLM_projects/ai_warehouse/datasets/BelleGroup___train_0.5_m_cn/default/0.0.0/29c35e9b56c7fc91f04a613ed33896f7a19bad54/BelleGroup___train_0.5_m_cn-train.arrow"

# ç”±äº arrow æ ¼å¼éœ€è¦è½¬æ¢ï¼Œå…ˆå°è¯•æ‰¾ json æ–‡ä»¶
BELLE_JSON=$(find /mnt/c/AI_LLM_projects/ai_warehouse/datasets -name "*.json" | grep -i belle | head -1)

if [ -z "$BELLE_JSON" ]; then
    echo -e "${YELLOW}âš ï¸  Belle JSON not found, trying Firefly dataset...${NC}"
    FIREFLY_JSON=$(find /mnt/c/AI_LLM_projects/ai_warehouse/datasets -name "*.json" | grep -i firefly | head -1)

    if [ -z "$FIREFLY_JSON" ]; then
        echo -e "${YELLOW}âš ï¸  No suitable dataset found. Skipping general data selection.${NC}"
        echo -e "${YELLOW}Will use only fandom data for training.${NC}"
        cp data/training/fandom-first-person-training-data.jsonl data/training/final-training-data.jsonl
    else
        python3 scripts/select-quality-general-data.py \
            --dataset "$FIREFLY_JSON" \
            --output data/training/general-conversation-subset.jsonl \
            --count 300 \
            --min-length 10 \
            --max-length 500
    fi
else
    python3 scripts/select-quality-general-data.py \
        --dataset "$BELLE_JSON" \
        --output data/training/general-conversation-subset.jsonl \
        --count 300 \
        --min-length 10 \
        --max-length 500
fi

# Step 3: åˆå¹¶æ•°æ®é›†
echo -e "\n${GREEN}[Step 3/3] åˆå¹¶æ•°æ®é›†...${NC}\n"

if [ -f "data/training/general-conversation-subset.jsonl" ]; then
    cat data/training/fandom-first-person-training-data.jsonl \
        data/training/general-conversation-subset.jsonl \
        > data/training/final-training-data.jsonl

    general_count=$(wc -l < data/training/general-conversation-subset.jsonl)
    total_count=$(wc -l < data/training/final-training-data.jsonl)
    fandom_percent=$((fandom_count * 100 / total_count))
    general_percent=$((general_count * 100 / total_count))

    echo -e "${GREEN}âœ… æ•°æ®é›†åˆå¹¶å®Œæˆï¼${NC}"
    echo -e "\n${BLUE}ğŸ“Š æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡ï¼š${NC}"
    echo -e "  Fandom æ•°æ®: ${fandom_count} æ ·æœ¬ (${fandom_percent}%)"
    echo -e "  ä¸€èˆ¬å¯¹è¯æ•°æ®: ${general_count} æ ·æœ¬ (${general_percent}%)"
    echo -e "  æ€»è®¡: ${total_count} æ ·æœ¬"
else
    echo -e "${YELLOW}âš ï¸  No general data, using only fandom data${NC}"
    total_count=$fandom_count
fi

# æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶ä¿¡æ¯
echo -e "\n${GREEN}âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆï¼${NC}"
echo -e "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${GREEN}ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š${NC}"
echo -e "  data/training/fandom-first-person-training-data.jsonl (${fandom_count} samples)"
if [ -f "data/training/general-conversation-subset.jsonl" ]; then
    echo -e "  data/training/general-conversation-subset.jsonl (${general_count} samples)"
fi
echo -e "  ${YELLOW}data/training/final-training-data.jsonl (${total_count} samples)${NC} â† ç”¨äº fine-tuning"
echo -e "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}\n"

# ä¸‹ä¸€æ­¥æŒ‡å¼•
echo -e "${GREEN}ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šFine-tuning${NC}"
echo -e "1. å¤åˆ¶æ•°æ®åˆ° AI warehouse:"
echo -e "   ${BLUE}cp data/training/final-training-data.jsonl \\"
echo -e "      /mnt/c/AI_LLM_projects/ai_warehouse/datasets/elio-fandom-complete.json${NC}\n"
echo -e "2. è¿è¡Œ fine-tuning:"
echo -e "   ${BLUE}cd ai-service${NC}"
echo -e "   ${BLUE}python scripts/train_sft.py \\"
echo -e "      --dataset /mnt/c/AI_LLM_projects/ai_warehouse/datasets/elio-fandom-complete.json \\"
echo -e "      --output_dir ./models/elio-fandom-complete \\"
echo -e "      --num_epochs 3${NC}\n"

echo -e "${GREEN}ğŸ‰ All done!${NC}\n"
